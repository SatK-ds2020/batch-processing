{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26892885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2a27c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d600f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/suman/spark/spark-3.3.2-bin-hadoop3/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b341ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10cf1119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 21:57:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d45dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-28 21:58:04--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.154.99.225, 18.154.99.47, 18.154.99.220, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.154.99.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-02-28 21:58:04 (190 MB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16117b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cbf15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e58250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ddd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "470fa9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d425e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28K\r\n",
      "-rw-rw-r-- 1 suman suman 6.3K Feb 28 22:05 pyspark-init.ipynb\r\n",
      "-rw-rw-r-- 1 suman suman  13K Feb 22  2024 taxi_zone_lookup.csv\r\n",
      "drwxr-xr-x 2 suman suman 4.0K Feb 28 21:58 zones\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666c3851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-28 22:19:00--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250228T221900Z&X-Amz-Expires=300&X-Amz-Signature=f44e9f1d683346c53e629dde1a0f4b61b7072582f0997dc76b6380e9a3c6f8c0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-02-28 22:19:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250228%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250228T221900Z&X-Amz-Expires=300&X-Amz-Signature=f44e9f1d683346c53e629dde1a0f4b61b7072582f0997dc76b6380e9a3c6f8c0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 129967421 (124M) [application/octet-stream]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.csv.gz’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 123.95M   153MB/s    in 0.8s    \n",
      "\n",
      "2025-02-28 22:19:01 (153 MB/s) - ‘fhvhv_tripdata_2021-01.csv.gz’ saved [129967421/129967421]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a039a2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11908469 fhvhv_tripdata_2021-01.csv\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "771e1257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:37:45 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "25/02/28 22:37:45 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/02/28 22:37:45 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/28 22:37:45 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#55, None)) > 0)\n",
      "25/02/28 22:37:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "25/02/28 22:37:45 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.3 KiB, free 434.2 MiB)\n",
      "25/02/28 22:37:45 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)\n",
      "25/02/28 22:37:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 (size: 34.0 KiB, free: 434.4 MiB)\n",
      "25/02/28 22:37:45 INFO SparkContext: Created broadcast 7 from csv at NativeMethodAccessorImpl.java:0\n",
      "25/02/28 22:37:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/28 22:37:45 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/02/28 22:37:45 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 11.8 KiB, free 434.2 MiB)\n",
      "25/02/28 22:37:45 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.2 MiB)\n",
      "25/02/28 22:37:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "25/02/28 22:37:45 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/28 22:37:45 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/02/28 22:37:45 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 0, PROCESS_LOCAL, 4930 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:37:45 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "25/02/28 22:37:45 INFO FileScanRDD: Reading File path: file:///home/suman/batch-pyspark/fhvhv_tripdata_2021-01.csv, range: 0-134217728, partition values: [empty row]\n",
      "25/02/28 22:37:45 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1614 bytes result sent to driver\n",
      "25/02/28 22:37:45 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 22 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (1/1)\n",
      "25/02/28 22:37:45 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/02/28 22:37:45 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.037 s\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/28 22:37:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "25/02/28 22:37:45 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.044929 s\n",
      "25/02/28 22:37:45 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/28 22:37:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/28 22:37:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "25/02/28 22:37:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 199.3 KiB, free 434.0 MiB)\n",
      "25/02/28 22:37:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.9 MiB)\n",
      "25/02/28 22:37:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 (size: 34.0 KiB, free: 434.3 MiB)\n",
      "25/02/28 22:37:45 INFO SparkContext: Created broadcast 9 from csv at NativeMethodAccessorImpl.java:0\n",
      "25/02/28 22:37:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff03645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21af998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6caa9192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 head.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aadc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d160514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d39160b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4037492",
   "metadata": {},
   "source": [
    "# Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b30523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdf8f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42b12d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05656f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:41:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d7ffe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0dfcb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:42:46 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/28 22:42:46 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/28 22:42:46 INFO FileSourceStrategy: Output Data Schema: struct<hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int ... 5 more fields>\n",
      "25/02/28 22:42:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:42:46 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 199.2 KiB, free 433.7 MiB)\n",
      "25/02/28 22:42:46 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.7 MiB)\n",
      "25/02/28 22:42:46 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 (size: 34.0 KiB, free: 434.3 MiB)\n",
      "25/02/28 22:42:46 INFO SparkContext: Created broadcast 10 from parquet at NativeMethodAccessorImpl.java:0\n",
      "25/02/28 22:42:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/28 22:42:46 INFO DAGScheduler: Registering RDD 33 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/02/28 22:42:46 INFO DAGScheduler: Got map stage job 4 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/02/28 22:42:46 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "25/02/28 22:42:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/28 22:42:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/28 22:42:46 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[33] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/02/28 22:42:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 14.0 KiB, free 433.7 MiB)\n",
      "25/02/28 22:42:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.9 MiB)\n",
      "25/02/28 22:42:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 (size: 7.4 KiB, free: 434.3 MiB)\n",
      "25/02/28 22:42:47 INFO BlockManagerInfo: Removed broadcast_7_piece0 on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 in memory (size: 34.0 KiB, free: 434.3 MiB)\n",
      "25/02/28 22:42:47 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
      "25/02/28 22:42:47 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[33] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/02/28 22:42:47 INFO TaskSchedulerImpl: Adding task set 4.0 with 6 tasks resource profile 0\n",
      "25/02/28 22:42:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 0, PROCESS_LOCAL, 4919 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:42:47 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 1, PROCESS_LOCAL, 4919 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:42:47 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 6) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 2, PROCESS_LOCAL, 4919 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:42:47 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 7) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 3, PROCESS_LOCAL, 4919 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:42:47 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "25/02/28 22:42:47 INFO Executor: Running task 2.0 in stage 4.0 (TID 6)\n",
      "25/02/28 22:42:47 INFO BlockManagerInfo: Removed broadcast_9_piece0 on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 in memory (size: 34.0 KiB, free: 434.4 MiB)\n",
      "25/02/28 22:42:47 INFO Executor: Running task 1.0 in stage 4.0 (TID 5)\n",
      "25/02/28 22:42:47 INFO Executor: Running task 3.0 in stage 4.0 (TID 7)\n",
      "25/02/28 22:42:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "25/02/28 22:42:47 INFO FileScanRDD: Reading File path: file:///home/suman/batch-pyspark/fhvhv_tripdata_2021-01.csv, range: 134217728-268435456, partition values: [empty row]\n",
      "25/02/28 22:42:47 INFO FileScanRDD: Reading File path: file:///home/suman/batch-pyspark/fhvhv_tripdata_2021-01.csv, range: 268435456-402653184, partition values: [empty row]\n",
      "25/02/28 22:42:47 INFO FileScanRDD: Reading File path: file:///home/suman/batch-pyspark/fhvhv_tripdata_2021-01.csv, range: 402653184-536870912, partition values: [empty row]\n",
      "25/02/28 22:42:47 INFO FileScanRDD: Reading File path: file:///home/suman/batch-pyspark/fhvhv_tripdata_2021-01.csv, range: 0-134217728, partition values: [empty row]\n",
      "25/02/28 22:42:47 INFO CodeGenerator: Code generated in 46.304011 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 4) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:42:54 INFO UnsafeExternalSorter: Thread 129 spilling sort data of 108.0 MiB to disk (0  time so far)\n",
      "25/02/28 22:42:54 INFO UnsafeExternalSorter: Thread 130 spilling sort data of 108.0 MiB to disk (0  time so far)\n",
      "25/02/28 22:42:55 INFO UnsafeExternalSorter: Thread 131 spilling sort data of 108.0 MiB to disk (0  time so far)\n",
      "25/02/28 22:42:55 INFO UnsafeExternalSorter: Thread 127 spilling sort data of 108.0 MiB to disk (0  time so far)\n",
      "25/02/28 22:43:02 INFO UnsafeExternalSorter: Thread 129 spilling sort data of 108.0 MiB to disk (1  time so far)\n",
      "25/02/28 22:43:02 INFO UnsafeExternalSorter: Thread 131 spilling sort data of 108.0 MiB to disk (1  time so far)\n",
      "25/02/28 22:43:03 INFO UnsafeExternalSorter: Thread 130 spilling sort data of 108.0 MiB to disk (1  time so far)\n",
      "25/02/28 22:43:03 INFO UnsafeExternalSorter: Thread 127 spilling sort data of 108.0 MiB to disk (1  time so far)\n",
      "25/02/28 22:43:09 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1951 bytes result sent to driver\n",
      "25/02/28 22:43:09 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 8) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 4, PROCESS_LOCAL, 4919 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:09 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 22075 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (1/6)\n",
      "25/02/28 22:43:09 INFO Executor: Running task 4.0 in stage 4.0 (TID 8)\n",
      "25/02/28 22:43:09 INFO Executor: Finished task 2.0 in stage 4.0 (TID 6). 1951 bytes result sent to driver\n",
      "25/02/28 22:43:09 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 9) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 5, PROCESS_LOCAL, 4919 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:09 INFO Executor: Running task 5.0 in stage 4.0 (TID 9)\n",
      "25/02/28 22:43:09 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 6) in 22084 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (2/6)\n",
      "25/02/28 22:43:09 INFO FileScanRDD: Reading File path: file:///home/suman/batch-pyspark/fhvhv_tripdata_2021-01.csv, range: 671088640-752335705, partition values: [empty row]\n",
      "25/02/28 22:43:09 INFO FileScanRDD: Reading File path: file:///home/suman/batch-pyspark/fhvhv_tripdata_2021-01.csv, range: 536870912-671088640, partition values: [empty row]\n",
      "25/02/28 22:43:09 INFO Executor: Finished task 1.0 in stage 4.0 (TID 5). 1908 bytes result sent to driver\n",
      "25/02/28 22:43:09 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 5) in 22116 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (3/6)\n",
      "25/02/28 22:43:09 INFO Executor: Finished task 3.0 in stage 4.0 (TID 7). 1908 bytes result sent to driver\n",
      "25/02/28 22:43:09 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 7) in 22127 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (4/6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:15 INFO UnsafeExternalSorter: Thread 127 spilling sort data of 216.0 MiB to disk (0  time so far)\n",
      "25/02/28 22:43:15 INFO Executor: Finished task 5.0 in stage 4.0 (TID 9). 1822 bytes result sent to driver\n",
      "25/02/28 22:43:15 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 9) in 6517 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (5/6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:19 INFO Executor: Finished task 4.0 in stage 4.0 (TID 8). 1908 bytes result sent to driver\n",
      "25/02/28 22:43:19 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 8) in 10315 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (6/6)\n",
      "25/02/28 22:43:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/02/28 22:43:19 INFO DAGScheduler: ShuffleMapStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 32.473 s\n",
      "25/02/28 22:43:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/02/28 22:43:19 INFO DAGScheduler: running: Set()\n",
      "25/02/28 22:43:19 INFO DAGScheduler: waiting: Set()\n",
      "25/02/28 22:43:19 INFO DAGScheduler: failed: Set()\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "25/02/28 22:43:19 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 24 output partitions\n",
      "25/02/28 22:43:19 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "25/02/28 22:43:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "25/02/28 22:43:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/28 22:43:19 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[34] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/02/28 22:43:19 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 208.3 KiB, free 433.9 MiB)\n",
      "25/02/28 22:43:19 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 74.6 KiB, free 433.9 MiB)\n",
      "25/02/28 22:43:19 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 (size: 74.6 KiB, free: 434.3 MiB)\n",
      "25/02/28 22:43:19 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "25/02/28 22:43:19 INFO DAGScheduler: Submitting 24 missing tasks from ResultStage 6 (ShuffledRowRDD[34] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "25/02/28 22:43:19 INFO TaskSchedulerImpl: Adding task set 6.0 with 24 tasks resource profile 0\n",
      "25/02/28 22:43:19 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 10) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:19 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 11) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:19 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 12) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:19 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 13) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:19 INFO Executor: Running task 0.0 in stage 6.0 (TID 10)\n",
      "25/02/28 22:43:19 INFO Executor: Running task 1.0 in stage 6.0 (TID 11)\n",
      "25/02/28 22:43:19 INFO Executor: Running task 2.0 in stage 6.0 (TID 12)\n",
      "25/02/28 22:43:19 INFO Executor: Running task 3.0 in stage 6.0 (TID 13)\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 39 ms\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:19 INFO BlockManagerInfo: Removed broadcast_11_piece0 on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 in memory (size: 7.4 KiB, free: 434.3 MiB)\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 44 ms\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 46 ms\n",
      "25/02/28 22:43:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 45 ms\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:19 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:19 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:19 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/02/28 22:43:19 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/02/28 22:43:19 INFO CodecPool: Got brand-new compressor [.snappy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                         (0 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:23 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000002_12' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000002\n",
      "25/02/28 22:43:23 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000002_12: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000003_13' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000003\n",
      "25/02/28 22:43:23 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000003_13: Committed. Elapsed time: 2 ms.\n",
      "25/02/28 22:43:23 INFO Executor: Finished task 2.0 in stage 6.0 (TID 12). 3534 bytes result sent to driver\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 14) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 4, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:23 INFO Executor: Running task 4.0 in stage 6.0 (TID 14)\n",
      "25/02/28 22:43:23 INFO Executor: Finished task 3.0 in stage 6.0 (TID 13). 3534 bytes result sent to driver\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 12) in 4120 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (1/24)\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 15) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 5, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:23 INFO Executor: Running task 5.0 in stage 6.0 (TID 15)\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 13) in 4123 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (2/24)\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000000_10' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000000\n",
      "25/02/28 22:43:23 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000000_10: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:23 INFO Executor: Finished task 0.0 in stage 6.0 (TID 10). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 16) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 6, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:23 INFO Executor: Running task 6.0 in stage 6.0 (TID 16)\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 10) in 4176 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (3/24)\n",
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 30 ms\n",
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000001_11' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000001\n",
      "25/02/28 22:43:23 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000001_11: Committed. Elapsed time: 3 ms.\n",
      "25/02/28 22:43:23 INFO Executor: Finished task 1.0 in stage 6.0 (TID 11). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 17) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 7, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:23 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 11) in 4253 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (4/24)\n",
      "25/02/28 22:43:23 INFO Executor: Running task 7.0 in stage 6.0 (TID 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=========>                                                (4 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:23 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000007_17' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000007\n",
      "25/02/28 22:43:26 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000007_17: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:26 INFO Executor: Finished task 7.0 in stage 6.0 (TID 17). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:26 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 18) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 8, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:26 INFO Executor: Running task 8.0 in stage 6.0 (TID 18)\n",
      "25/02/28 22:43:26 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 17) in 2871 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (5/24)\n",
      "25/02/28 22:43:26 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:26 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:26 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000005_15' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000005\n",
      "25/02/28 22:43:26 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000005_15: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:26 INFO Executor: Finished task 5.0 in stage 6.0 (TID 15). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:26 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 19) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 9, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:26 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 15) in 3120 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (6/24)\n",
      "25/02/28 22:43:26 INFO Executor: Running task 9.0 in stage 6.0 (TID 19)\n",
      "25/02/28 22:43:26 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:26 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:26 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:26 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:================>                                         (7 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:27 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000006_16' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000006\n",
      "25/02/28 22:43:27 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000006_16: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:27 INFO Executor: Finished task 6.0 in stage 6.0 (TID 16). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:27 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 20) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 10, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:27 INFO Executor: Running task 10.0 in stage 6.0 (TID 20)\n",
      "25/02/28 22:43:27 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 16) in 3215 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (7/24)\n",
      "25/02/28 22:43:27 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000004_14' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000004\n",
      "25/02/28 22:43:27 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000004_14: Committed. Elapsed time: 2 ms.\n",
      "25/02/28 22:43:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:27 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:27 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:27 INFO Executor: Finished task 4.0 in stage 6.0 (TID 14). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:27 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 21) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 11, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:27 INFO Executor: Running task 11.0 in stage 6.0 (TID 21)\n",
      "25/02/28 22:43:27 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 14) in 3320 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (8/24)\n",
      "25/02/28 22:43:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:27 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:27 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:27 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:27 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================>                                      (8 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000008_18' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000008\n",
      "25/02/28 22:43:29 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000008_18: Committed. Elapsed time: 2 ms.\n",
      "25/02/28 22:43:29 INFO Executor: Finished task 8.0 in stage 6.0 (TID 18). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 22) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 12, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 18) in 2307 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (9/24)\n",
      "25/02/28 22:43:29 INFO Executor: Running task 12.0 in stage 6.0 (TID 22)\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000009_19' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000009\n",
      "25/02/28 22:43:29 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000009_19: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:29 INFO Executor: Finished task 9.0 in stage 6.0 (TID 19). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 23) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 13, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:29 INFO Executor: Running task 13.0 in stage 6.0 (TID 23)\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 19) in 2272 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (10/24)\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================>                            (12 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000011_21' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000011\n",
      "25/02/28 22:43:29 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000011_21: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:29 INFO Executor: Finished task 11.0 in stage 6.0 (TID 21). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 24) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 14, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:29 INFO Executor: Running task 14.0 in stage 6.0 (TID 24)\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 21) in 2263 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (11/24)\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000010_20' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000010\n",
      "25/02/28 22:43:29 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000010_20: Committed. Elapsed time: 0 ms.\n",
      "25/02/28 22:43:29 INFO Executor: Finished task 10.0 in stage 6.0 (TID 20). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 25) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 15, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:29 INFO Executor: Running task 15.0 in stage 6.0 (TID 25)\n",
      "25/02/28 22:43:29 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 20) in 2349 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (12/24)\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:29 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000013_23' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000013\n",
      "25/02/28 22:43:31 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000013_23: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:31 INFO Executor: Finished task 13.0 in stage 6.0 (TID 23). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:31 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 26) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 16, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:31 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 23) in 2444 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (13/24)\n",
      "25/02/28 22:43:31 INFO Executor: Running task 16.0 in stage 6.0 (TID 26)\n",
      "25/02/28 22:43:31 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:31 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:31 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000014_24' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000014\n",
      "25/02/28 22:43:31 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000014_24: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:31 INFO Executor: Finished task 14.0 in stage 6.0 (TID 24). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:31 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 27) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 17, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:31 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 24) in 2326 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (14/24)\n",
      "25/02/28 22:43:31 INFO Executor: Running task 17.0 in stage 6.0 (TID 27)\n",
      "25/02/28 22:43:31 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:31 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:31 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000012_22' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000012\n",
      "25/02/28 22:43:31 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000012_22: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:31 INFO Executor: Finished task 12.0 in stage 6.0 (TID 22). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:31 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 28) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 18, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:31 INFO Executor: Running task 18.0 in stage 6.0 (TID 28)\n",
      "25/02/28 22:43:31 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 22) in 2701 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (15/24)\n",
      "25/02/28 22:43:31 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:31 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:31 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Parquet block size to 134217728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================>                     (15 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:31 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:32 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000015_25' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000015\n",
      "25/02/28 22:43:32 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000015_25: Committed. Elapsed time: 12 ms.\n",
      "25/02/28 22:43:32 INFO Executor: Finished task 15.0 in stage 6.0 (TID 25). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:32 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 29) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 19, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:32 INFO Executor: Running task 19.0 in stage 6.0 (TID 29)\n",
      "25/02/28 22:43:32 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 25) in 2695 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (16/24)\n",
      "25/02/28 22:43:32 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:32 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:32 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:32 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:32 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:32 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================>                  (16 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:33 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000016_26' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000016\n",
      "25/02/28 22:43:33 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000016_26: Committed. Elapsed time: 11 ms.\n",
      "25/02/28 22:43:33 INFO Executor: Finished task 16.0 in stage 6.0 (TID 26). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:33 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 30) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 20, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:33 INFO Executor: Running task 20.0 in stage 6.0 (TID 30)\n",
      "25/02/28 22:43:33 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 26) in 2338 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (17/24)\n",
      "25/02/28 22:43:33 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:33 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:33 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:33 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:33 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:33 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000017_27' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000017\n",
      "25/02/28 22:43:33 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000017_27: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:33 INFO Executor: Finished task 17.0 in stage 6.0 (TID 27). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:33 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 31) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 21, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:33 INFO Executor: Running task 21.0 in stage 6.0 (TID 31)\n",
      "25/02/28 22:43:33 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 27) in 2317 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (18/24)\n",
      "25/02/28 22:43:33 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:33 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:33 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===============================================>         (20 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:34 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000018_28' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000018\n",
      "25/02/28 22:43:34 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000018_28: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:34 INFO Executor: Finished task 18.0 in stage 6.0 (TID 28). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:34 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 32) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 22, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:34 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 28) in 2408 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (19/24)\n",
      "25/02/28 22:43:34 INFO Executor: Running task 22.0 in stage 6.0 (TID 32)\n",
      "25/02/28 22:43:34 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:34 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:34 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000019_29' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000019\n",
      "25/02/28 22:43:34 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000019_29: Committed. Elapsed time: 2 ms.\n",
      "25/02/28 22:43:34 INFO Executor: Finished task 19.0 in stage 6.0 (TID 29). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:34 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 33) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 23, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:43:34 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 29) in 2180 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (20/24)\n",
      "25/02/28 22:43:34 INFO Executor: Running task 23.0 in stage 6.0 (TID 33)\n",
      "25/02/28 22:43:34 INFO ShuffleBlockFetcherIterator: Getting 6 (14.0 MiB) non-empty blocks including 6 (14.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/28 22:43:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/02/28 22:43:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/02/28 22:43:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/02/28 22:43:34 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:34 INFO CodecConfig: Compression: SNAPPY\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Validation is off\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/02/28 22:43:34 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/02/28 22:43:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"hvfhs_license_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dispatching_base_num\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"pickup_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dropoff_datetime\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"PULocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DOLocationID\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SR_Flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary hvfhs_license_num (STRING);\n",
      "  optional binary dispatching_base_num (STRING);\n",
      "  optional int96 pickup_datetime;\n",
      "  optional int96 dropoff_datetime;\n",
      "  optional int32 PULocationID;\n",
      "  optional int32 DOLocationID;\n",
      "  optional binary SR_Flag (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/02/28 22:43:36 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000021_31' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000021\n",
      "25/02/28 22:43:36 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000021_31: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:36 INFO Executor: Finished task 21.0 in stage 6.0 (TID 31). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:36 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 31) in 2304 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (21/24)\n",
      "25/02/28 22:43:36 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000020_30' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000020\n",
      "25/02/28 22:43:36 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000020_30: Committed. Elapsed time: 0 ms.\n",
      "25/02/28 22:43:36 INFO Executor: Finished task 20.0 in stage 6.0 (TID 30). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:36 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 30) in 2377 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (22/24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:43:36 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000022_32' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000022\n",
      "25/02/28 22:43:36 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000022_32: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:36 INFO Executor: Finished task 22.0 in stage 6.0 (TID 32). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:36 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 32) in 2301 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (23/24)\n",
      "25/02/28 22:43:36 INFO FileOutputCommitter: Saved output of task 'attempt_20250228224319464107132663381948_0006_m_000023_33' to file:/home/suman/batch-pyspark/fhvhv/2021/01/_temporary/0/task_20250228224319464107132663381948_0006_m_000023\n",
      "25/02/28 22:43:36 INFO SparkHadoopMapRedUtil: attempt_20250228224319464107132663381948_0006_m_000023_33: Committed. Elapsed time: 1 ms.\n",
      "25/02/28 22:43:36 INFO Executor: Finished task 23.0 in stage 6.0 (TID 33). 3491 bytes result sent to driver\n",
      "25/02/28 22:43:36 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 33) in 2349 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (24/24)\n",
      "25/02/28 22:43:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "25/02/28 22:43:36 INFO DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 16.986 s\n",
      "25/02/28 22:43:36 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/28 22:43:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "25/02/28 22:43:36 INFO DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 17.006255 s\n",
      "25/02/28 22:43:36 INFO FileFormatWriter: Start to commit write Job 37e49070-cb85-49fc-b4fa-d9fd885c3f8e.\n",
      "25/02/28 22:43:36 INFO FileFormatWriter: Write Job 37e49070-cb85-49fc-b4fa-d9fd885c3f8e committed. Elapsed time: 32 ms.\n",
      "25/02/28 22:43:36 INFO FileFormatWriter: Finished processing stats for write job 37e49070-cb85-49fc-b4fa-d9fd885c3f8e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66864ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/28 22:44:41 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
      "25/02/28 22:44:41 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[36] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/02/28 22:44:41 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 102.8 KiB, free 433.8 MiB)\n",
      "25/02/28 22:44:41 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 433.8 MiB)\n",
      "25/02/28 22:44:41 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal:38509 (size: 36.8 KiB, free: 434.3 MiB)\n",
      "25/02/28 22:44:41 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[36] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/28 22:44:41 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "25/02/28 22:44:41 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 34) (de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal, executor driver, partition 0, PROCESS_LOCAL, 4660 bytes) taskResourceAssignments Map()\n",
      "25/02/28 22:44:41 INFO Executor: Running task 0.0 in stage 7.0 (TID 34)\n",
      "25/02/28 22:44:41 INFO Executor: Finished task 0.0 in stage 7.0 (TID 34). 2002 bytes result sent to driver\n",
      "25/02/28 22:44:41 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 34) in 81 ms on de-vm2025.us-central1-c.c.de-zoomcamp2025-448100.internal (executor driver) (1/1)\n",
      "25/02/28 22:44:41 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "25/02/28 22:44:41 INFO DAGScheduler: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.096 s\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/28 22:44:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "25/02/28 22:44:41 INFO DAGScheduler: Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.101213 s\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7a99a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38fcff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d68283b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41fe9a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2021-01-03 17:17:21|2021-01-03 17:26:18|         255|          34|   null|\n",
      "|           HV0003|              B02882|2021-01-05 22:14:07|2021-01-05 22:32:28|         189|         107|   null|\n",
      "|           HV0003|              B02867|2021-01-02 17:59:55|2021-01-02 18:10:39|          88|         137|   null|\n",
      "|           HV0003|              B02872|2021-01-02 23:57:54|2021-01-03 00:15:48|         238|         224|   null|\n",
      "|           HV0003|              B02875|2021-01-06 15:53:13|2021-01-06 16:07:07|         169|         208|   null|\n",
      "|           HV0003|              B02867|2021-01-07 07:35:24|2021-01-07 07:55:49|          75|          88|   null|\n",
      "|           HV0003|              B02764|2021-01-07 08:45:12|2021-01-07 08:51:17|         210|         210|   null|\n",
      "|           HV0003|              B02764|2021-01-02 15:44:26|2021-01-02 16:10:50|         243|          69|   null|\n",
      "|           HV0003|              B02869|2021-01-04 16:50:28|2021-01-04 16:57:43|         250|         213|   null|\n",
      "|           HV0003|              B02877|2021-01-03 10:30:34|2021-01-03 10:44:53|          87|          79|   null|\n",
      "|           HV0003|              B02617|2021-01-03 22:05:20|2021-01-03 22:27:55|          68|         181|   null|\n",
      "|           HV0003|              B02765|2021-01-04 08:01:02|2021-01-04 08:33:27|          95|         236|   null|\n",
      "|           HV0003|              B02835|2021-01-02 13:01:10|2021-01-02 13:08:11|         262|         236|   null|\n",
      "|           HV0005|              B02510|2021-01-04 05:25:51|2021-01-04 05:45:19|         225|         233|   null|\n",
      "|           HV0003|              B02836|2021-01-06 17:12:27|2021-01-06 17:46:56|         237|          83|   null|\n",
      "|           HV0005|              B02510|2021-01-05 07:07:33|2021-01-05 07:16:16|         231|          87|   null|\n",
      "|           HV0005|              B02510|2021-01-06 11:21:01|2021-01-06 11:31:58|          22|          26|   null|\n",
      "|           HV0003|              B02682|2021-01-04 09:05:18|2021-01-04 09:27:50|         159|          75|   null|\n",
      "|           HV0003|              B02869|2021-01-06 16:46:47|2021-01-06 17:50:24|         109|         119|   null|\n",
      "|           HV0003|              B02883|2021-01-06 08:03:47|2021-01-06 08:17:43|         145|         229|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "326f828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9d68764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02884')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3717917",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b853366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         255|          34|\n",
      "|  e/b42| 2021-01-05|  2021-01-05|         189|         107|\n",
      "|  e/b33| 2021-01-02|  2021-01-02|          88|         137|\n",
      "|  e/b38| 2021-01-02|  2021-01-03|         238|         224|\n",
      "|  e/b3b| 2021-01-06|  2021-01-06|         169|         208|\n",
      "|  e/b33| 2021-01-07|  2021-01-07|          75|          88|\n",
      "|  e/acc| 2021-01-07|  2021-01-07|         210|         210|\n",
      "|  e/acc| 2021-01-02|  2021-01-02|         243|          69|\n",
      "|  e/b35| 2021-01-04|  2021-01-04|         250|         213|\n",
      "|  s/b3d| 2021-01-03|  2021-01-03|          87|          79|\n",
      "|  e/a39| 2021-01-03|  2021-01-03|          68|         181|\n",
      "|  s/acd| 2021-01-04|  2021-01-04|          95|         236|\n",
      "|  s/b13| 2021-01-02|  2021-01-02|         262|         236|\n",
      "|  e/9ce| 2021-01-04|  2021-01-04|         225|         233|\n",
      "|  e/b14| 2021-01-06|  2021-01-06|         237|          83|\n",
      "|  e/9ce| 2021-01-05|  2021-01-05|         231|          87|\n",
      "|  e/9ce| 2021-01-06|  2021-01-06|          22|          26|\n",
      "|  a/a7a| 2021-01-04|  2021-01-04|         159|          75|\n",
      "|  e/b35| 2021-01-06|  2021-01-06|         109|         119|\n",
      "|  a/b43| 2021-01-06|  2021-01-06|         145|         229|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6b86390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 5, 22, 14, 7), dropoff_datetime=datetime.datetime(2021, 1, 5, 22, 32, 28), PULocationID=189, DOLocationID=107),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 17, 59, 55), dropoff_datetime=datetime.datetime(2021, 1, 2, 18, 10, 39), PULocationID=88, DOLocationID=137),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 23, 57, 54), dropoff_datetime=datetime.datetime(2021, 1, 3, 0, 15, 48), PULocationID=238, DOLocationID=224),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 15, 53, 13), dropoff_datetime=datetime.datetime(2021, 1, 6, 16, 7, 7), PULocationID=169, DOLocationID=208),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 7, 7, 35, 24), dropoff_datetime=datetime.datetime(2021, 1, 7, 7, 55, 49), PULocationID=75, DOLocationID=88)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV0003').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58008c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
